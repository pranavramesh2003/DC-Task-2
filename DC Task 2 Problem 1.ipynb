{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0c26a9",
   "metadata": {},
   "source": [
    "We use the Taylor's Theorem upto a linear approximation for two parameters $x_{0}$ and $x_{1}$ and the cost function defined as\n",
    "$$\n",
    "J(\\theta_{0},\\theta_{1})=\\frac{\\sum(y_{predicted}-y_{actual})^{2}}{2n}\n",
    "$$\n",
    "\n",
    "Let us assume that $J(\\theta_{0},\\theta_{1})$ converges, upon a suitable number of gradient descents, to $J(\\theta_{0}^\\infty,\\theta_{1}^\\infty)$. We need to show, for convergence, that after every iteration, that the function $J(\\theta_{0},\\theta_{1})$ is closer to $J(\\theta_{0}^\\infty,\\theta_{1}^\\infty)$ after a gradient descent than before it, i.e.\n",
    "\n",
    "$$ |{J(\\theta_{0}^\\infty,\\theta_{1}^\\infty)-J(\\theta_{0},\\theta_{1})}|>|{J(\\theta_{0}^\\infty,\\theta_{1}^\\infty)-J(\\theta_{0}^{1},\\theta_{1}^{1})}|$$\n",
    "\n",
    "where $J(\\theta_{0}^{1},\\theta_{1}^{1})$ is the value of J after the first gradient descent. This could be inductively extended since the $\\theta_{0},\\theta_{1}$ are arbitrary. \n",
    "\n",
    "Using the linear approximation of taylor's theorem, \n",
    "\n",
    "$$ {J(\\theta_{0}^{1},\\theta_{1}^{1})-J(\\theta_{0},\\theta_{1})}= \\frac {∂J}{∂\\theta_{0}}(\\theta_{0}^{1}-\\theta_{0}^{0})+\\frac {∂J}{∂\\theta_{0}}(\\theta_{1}^{1}-\\theta_{1}^{0})$$ \n",
    "\n",
    "Since gradient descent is designed as follows:\n",
    "\n",
    "$$ \\theta_{0}^{1}:\\theta_{0}-\\alpha\\frac {∂J}{∂\\theta_{0}}$$\n",
    "$$ \\theta_{1}^{1}:\\theta_{1}-\\alpha\\frac {∂J}{∂\\theta_{1}}$$\n",
    "\n",
    "The Taylor Linearization reduces to:\n",
    "\n",
    "$${J(\\theta_{0}^{1},\\theta_{1}^{1})-J(\\theta_{0},\\theta_{1})}=\\alpha \\frac {(\\frac {∂J}{∂\\theta_{0}})^2+(\\frac {∂J}{∂\\theta_{1}})^2}{n}  at (\\theta_{0},\\theta_{1})$$\n",
    "\n",
    "Similarly for the next gradient descent, \n",
    "\n",
    "$${J(\\theta_{0}^{2},\\theta_{1}^{2})-J(\\theta_{0}^{1},\\theta_{1}^{1})}=\\alpha \\frac {(\\frac {∂J}{∂\\theta_{0}})^2+(\\frac {∂J}{∂\\theta_{1}})^2}{n} at (\\theta_{0}^{1},\\theta_{1}^{1})$$\n",
    "\n",
    "Hence, the required inequality would reduce to \n",
    "\n",
    "$$\\sum_{0}^{\\infty} \\alpha\\frac {(\\frac {∂J}{∂\\theta_{0}})^2+(\\frac {∂J}{∂\\theta_{1}})^2}{n} >  \\sum_{1}^{\\infty} \\alpha\\frac {(\\frac {∂J}{∂\\theta_{0}})^2+(\\frac {∂J}{∂\\theta_{1}})^2}{n}$$\n",
    "\n",
    "where the partial derivatives are defined appropriately. This trivially true, since the terms added are perfect squares. The same idea could be extended to multiple parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511f85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
